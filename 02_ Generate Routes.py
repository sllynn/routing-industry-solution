# Databricks notebook source
# MAGIC %md 
# MAGIC You may find this series of notebooks at https://github.com/databricks-industry-solutions/routing.git. For more information about this solution accelerator, visit https://www.databricks.com/solutions/accelerators/scalable-route-generation.

# COMMAND ----------

# MAGIC %md The purpose of this notebook is to demonstrate how the the OSRM software running within a Databricks cluster can be used to generate routing data.  

# COMMAND ----------

# MAGIC %md ## Introduction
# MAGIC
# MAGIC With the software and map file assets in place, we have launched a cluster (with multiple worker nodes) that has deployed an instance of the OSRM Backend Server on each worker.
# MAGIC </p>
# MAGIC
# MAGIC <img src='https://brysmiwasb.blob.core.windows.net/demos/images/osrm_scaled_deployment2.png' width=500>
# MAGIC </p>
# MAGIC Using point data within a Spark dataframe which by default is distributed across these worker nodes, we can define a series of functions to make local calls to these server instances in order to generate routing data in a scalable manner.   

# COMMAND ----------

# DBTITLE 1,Install Required Libraries
# %pip install tabulate

# COMMAND ----------

# DBTITLE 1,Import Required Libraries
import requests

import pandas as pd
import numpy as np
import json

import itertools

import subprocess

import pyspark.sql.functions as F
from pyspark.sql.types import *

spark.conf.set("spark.databricks.geo.st.enabled", "true")
spark.conf.set("spark.sql.adaptive.enabled", "false")

# COMMAND ----------

# MAGIC %md ## Step 1: Verify Server Running on Each Worker
# MAGIC
# MAGIC Our first step is to verify the OSRM Backend Server is running on each worker node as expected.  To do this, we need to uncover the IP address used by each worker node in our cluster which we will do using an old-school Spark RDD which will force a small dataset to be distributed across the workers in our cluster.
# MAGIC
# MAGIC To better understand this, it's helpful to know that the memory and processor resources available on each worker node are divided between Java Virtual Machines (JVMs).  These JVMs are referred to as *Executors* and are responsible for housing a subset of the data in a Spark RDD or Spark dataframe.  There is typically a one-to-one relationship between an Executor and a worker node but this is not always the case.
# MAGIC
# MAGIC The [*sc.defaultParallelism*](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.SparkContext.defaultParallelism.html#pyspark.SparkContext.defaultParallelism) property keeps track of the number of processors availalbe across the worker nodes in a cluster, and by defining a Spark RDD using a parallelized range of values equivalent to this number, we are associating one integer value with each virtual core. The [*sc.runJob*](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.SparkContext.runJob.html) method then forces the the Python [*subprocess.run*](https://docs.python.org/3/library/subprocess.html#subprocess.run) method to run a local instance of the *hostname -I* command which retrieves the public IP address of the machine on which each of the value in the RDD. The output is returned as a list which is then transformed into a Python set to return just the unique IP values identified by the command.
# MAGIC
# MAGIC While that sounds like a lot of explanation for such a simple task, please note that this same pattern will be coming into play with a different type of function call later in this notebook:

# COMMAND ----------

# DBTITLE 1,Get Worker Node IP Addresses
# generate RDD to span each executor on each worker
myRDD = sc.parallelize(range(sc.defaultParallelism))

# get set of ip addresses
ip_addresses = set( # conversion to set deduplicates output
  sc.runJob(
    myRDD, 
    lambda _: [subprocess.run(['hostname','-I'], capture_output=True).stdout.decode('utf-8').strip()] # run hostname -I on each executor
    )
  )

ip_addresses

# COMMAND ----------

# MAGIC %md Now that we know the IP addresses of our worker nodes, we can quickly test the response of each OSRM Backend Server listening on default port 5000 by requesting a routing response from each:

# COMMAND ----------

# DBTITLE 1,Test Each Worker for a Routing Response
responses = []

# for each worker ip address
for ip in ip_addresses:
  
  # get a response from the osrm backend server
  resp = requests.get(f'http://{ip}:5000/route/v1/driving/-0.00080678,51.47781737;0.07229157,51.26704151').text
  responses += [(ip, resp)]
  
# display responses generated by each worker
display(
  pd.DataFrame(responses, columns=['ip','response'])
  )

# COMMAND ----------

pois_path = "file:/Workspace/Repos/stuart.lynn@databricks.com/routing/points-of-interest-exeter.csv"

read_options = dict(
  header=True,
  sep="|"
)

pois = (
  spark.read
  .format("csv")
  .options(**read_options)
  .load(pois_path)
  )

pois.display()

# COMMAND ----------

# %sql
# CREATE OR REPLACE FUNCTION BNGEASTNORTHTOPOINTGEOM(east DOUBLE, north DOUBLE)
# RETURNS BINARY
# RETURN ST_ASEWKB(ST_SETSRID(ST_POINT(east, north), 27700));

# CREATE OR REPLACE FUNCTION BNGGEOMTOWGS84(geom BINARY)
# RETURNS BINARY
# RETURN ST_ASEWKB(ST_TRANSFORM(geom, 4326));

# CREATE OR REPLACE FUNCTION BNGEASTNORTHTOLONLAT(east DOUBLE, north DOUBLE)
# RETURNS ARRAY<DOUBLE>
# RETURN ARRAY(
#   ST_X(BNGGEOMTOWGS84(BNGEASTNORTHTOPOINTGEOM(east, north))),
#   ST_Y(BNGGEOMTOWGS84(BNGEASTNORTHTOPOINTGEOM(east, north)))
#   );

# COMMAND ----------

# MAGIC %sql
# MAGIC CREATE OR REPLACE FUNCTION BNGEASTNORTHTOPOINTGEOM(east DOUBLE, north DOUBLE)
# MAGIC RETURNS GEOMETRY
# MAGIC RETURN ST_SETSRID(ST_POINT(east, north), 27700);
# MAGIC
# MAGIC CREATE OR REPLACE FUNCTION BNGGEOMTOWGS84(geom GEOMETRY)
# MAGIC RETURNS GEOMETRY
# MAGIC RETURN ST_TRANSFORM(geom, 4326);
# MAGIC
# MAGIC CREATE OR REPLACE FUNCTION BNGEASTNORTHTOLONLAT(east DOUBLE, north DOUBLE)
# MAGIC RETURNS ARRAY<DOUBLE>
# MAGIC RETURN ARRAY(
# MAGIC   ST_X(BNGGEOMTOWGS84(BNGEASTNORTHTOPOINTGEOM(east, north))),
# MAGIC   ST_Y(BNGGEOMTOWGS84(BNGEASTNORTHTOPOINTGEOM(east, north)))
# MAGIC   );

# COMMAND ----------

# MAGIC %sql SELECT BNGEASTNORTHTOLONLAT(291003.0, 92021.0)

# COMMAND ----------

key_cols = [
  "UNIQUE_REFERENCE_NUMBER",
  "NAME"
  ]

pois_4326 = (
  pois
  .withColumn("WGS84", F.expr("BNGEASTNORTHTOLONLAT(feature_easting, feature_northing)"))
  .select(
    *key_cols,
    F.expr("WGS84[0]").alias("lon"),
    F.expr("WGS84[1]").alias("lat")
    )
  )

pois_4326.display()

# COMMAND ----------

pois_4326.count()

# COMMAND ----------

location_pairs = (
  pois_4326.alias("start")
  .crossJoin(pois_4326.alias("end"))
  .where("start.UNIQUE_REFERENCE_NUMBER > end.UNIQUE_REFERENCE_NUMBER")
  .selectExpr(
    "start.UNIQUE_REFERENCE_NUMBER as start_urn",
    "start.NAME AS start_poi",
    "end.UNIQUE_REFERENCE_NUMBER as end_urn",
    "end.NAME AS end_poi",
    "start.lon AS start_lon",
    "start.lat AS start_lat",
    "end.lon AS end_lon",
    "end.lat AS end_lat"
    )
  )

location_pairs.display()

# COMMAND ----------

location_pairs.count()

# COMMAND ----------

# DBTITLE 1,Define Function to Get Route
@F.pandas_udf(StringType())
def get_osrm_route(
  start_longitudes: pd.Series, 
  start_latitudes:pd.Series, 
  end_longitudes: pd.Series, 
  end_latitudes: pd.Series
  ) -> pd.Series:
   
  # combine inputs to form dataframe
  df = pd.concat([start_longitudes, start_latitudes, end_longitudes, end_latitudes], axis=1)
  df.columns = ['start_lon','start_lat','end_lon','end_lat']

  # internal function to get route for a given row
  def _route(row):
    r = requests.get(
      f'http://127.0.0.1:5000/route/v1/driving/{row.start_lon},{row.start_lat};{row.end_lon},{row.end_lat}?alternatives=true&steps=false&geometries=geojson&overview=simplified&annotations=false'
    )
    return r.text
  
  # apply routing function row by row
  return df.apply(_route, axis=1)

# COMMAND ----------

# schema for the json document
response_schema = '''
  STRUCT<
    code: STRING, 
    routes: 
      ARRAY<
        STRUCT<
          distance: DOUBLE, 
          duration: DOUBLE, 
          geometry: STRUCT<
            coordinates: ARRAY<ARRAY<DOUBLE>>, 
            type: STRING
            >, 
          legs: ARRAY<
            STRUCT<
              distance: DOUBLE, 
              duration: DOUBLE, 
              steps: ARRAY<STRING>, 
              summary: STRING, 
              weight: DOUBLE
              >
            >, 
          weight: DOUBLE, 
          weight_name: STRING
          >
        >,
      waypoints: ARRAY<
        STRUCT<
          distance: DOUBLE, 
          hint: STRING, 
          location: ARRAY<DOUBLE>, 
          name: STRING
          >
        >
      >
  '''

# COMMAND ----------

routes = (
  location_pairs
  .repartition(spark.sparkContext.defaultParallelism*10)
  .withColumn(
      'osrm_route',
      F.from_json(get_osrm_route('start_lon','start_lat','end_lon','end_lat'), response_schema)
      )
  .withColumn("routes", F.explode("osrm_route.routes"))
  .withColumn("wkt_route", F.expr("ST_AsText(ST_GeomFromGeoJSON(To_JSON(routes.geometry)))"))
  .withColumn("distance_metres", F.col("routes.distance"))
  .withColumn("duration_seconds", F.col("routes.duration"))
  )
routes.display()

# COMMAND ----------

routes.write.mode("overwrite").saveAsTable("stuart.evri_geo.exeter_routes")

# COMMAND ----------

spark.table("stuart.evri_geo.exeter_routes").count()

# COMMAND ----------

dbutils.notebook.exit("0")

# COMMAND ----------

# MAGIC %md
# MAGIC
# MAGIC &copy; 2022 Databricks, Inc. All rights reserved. The source in this notebook is provided subject to the [Databricks License](https://databricks.com/db-license-source).  All included or referenced third party libraries are subject to the licenses set forth below.
# MAGIC
# MAGIC | library                                | description             | license    | source                                              |
# MAGIC |----------------------------------------|-------------------------|------------|-----------------------------------------------------|
# MAGIC | OSRM Backend Server                                  | High performance routing engine written in C++14 designed to run on OpenStreetMap data | BSD 2-Clause "Simplified" License    | https://github.com/Project-OSRM/osrm-backend                   |
# MAGIC | Mosaic | An extension to the Apache Spark framework that allows easy and fast processing of very large geospatial datasets | Databricks License| https://github.com/databrickslabs/mosaic | 
# MAGIC | Tabulate | pretty-print tabular data in Python | MIT License | https://pypi.org/project/tabulate/ |
